{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSIC_CSV_PATH = '/csvfiles/musiccaps-public.csv'\n",
    "TRAIN_DATA = 'data/train_data.pkl'\n",
    "TEST_DATA = 'data/test_data.pkl'\n",
    "VAL_DATA = 'data/val_data.pkl'\n",
    "URL_BASE = 'https://www.youtube.com/watch?v='\n",
    "MELS_DIR = '/MELS_DIR'\n",
    "AUDIO_DATA9 = 'audio_data9.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DATA = 'new_data/train_data.pkl'\n",
    "# TEST_DATA = 'new_data/test_data.pkl'\n",
    "# VAL_DATA = 'new_data/val_data.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_df = pd.read_pickle(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_audio_df = pd.read_pickle(VAL_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 431])\n"
     ]
    }
   ],
   "source": [
    "for row in val_audio_df.iterrows():\n",
    "    print(row[1]['mel_spec'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_mel_spec(mel_spec, target_shape=(128, 431)):\n",
    "    padded = torch.zeros(target_shape)\n",
    "    _, orig_width = mel_spec.shape\n",
    "    if orig_width < target_shape[1]:\n",
    "        padded[:, :orig_width] = mel_spec\n",
    "    else:\n",
    "        padded = mel_spec[:, :target_shape[1]]\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing mel spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_df['padded_mel_spec'] = train_audio_df['mel_spec'].apply(lambda x: pad_mel_spec(x))\n",
    "\n",
    "train_audio_df['normalized_mel_spec'] = train_audio_df['padded_mel_spec'].apply(lambda x: (x - x.min())/(x.max() - x.min()))\n",
    " \n",
    "mel_specs = torch.stack(train_audio_df['normalized_mel_spec'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    df['padded_mel_spec'] = df['mel_spec'].apply(lambda x: pad_mel_spec(x))\n",
    "    df['normalized_mel_spec'] = df['padded_mel_spec'].apply(lambda x: (x - x.min())/(x.max() - x.min()))\n",
    "    mel_specs = torch.stack(df['normalized_mel_spec'].tolist())\n",
    "    return mel_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_specs = preprocessing(train_audio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_spectos = preprocessing(val_audio_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5228, 0.4700, 0.4498,  ..., 0.5334, 0.5809, 0.5759],\n",
       "         [0.5017, 0.4436, 0.4273,  ..., 0.4969, 0.5210, 0.5317],\n",
       "         [0.5058, 0.4808, 0.4321,  ..., 0.3792, 0.3478, 0.4611],\n",
       "         ...,\n",
       "         [0.1467, 0.1820, 0.1790,  ..., 0.2137, 0.1656, 0.1671],\n",
       "         [0.0000, 0.0014, 0.0436,  ..., 0.2256, 0.2012, 0.2013],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.1955, 0.1961, 0.1996]],\n",
       "\n",
       "        [[0.5892, 0.6143, 0.6116,  ..., 0.3888, 0.3955, 0.3270],\n",
       "         [0.6252, 0.6526, 0.6681,  ..., 0.3610, 0.4184, 0.4135],\n",
       "         [0.7017, 0.7585, 0.7629,  ..., 0.5516, 0.5598, 0.5396],\n",
       "         ...,\n",
       "         [0.1917, 0.1395, 0.0772,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2126, 0.1619, 0.0972,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2067, 0.1494, 0.0671,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[0.7524, 0.7116, 0.6661,  ..., 0.6497, 0.7001, 0.7934],\n",
       "         [0.8362, 0.8619, 0.8477,  ..., 0.8314, 0.8450, 0.9279],\n",
       "         [0.8155, 0.8439, 0.8306,  ..., 0.8146, 0.9214, 0.9756],\n",
       "         ...,\n",
       "         [0.3263, 0.3492, 0.3044,  ..., 0.4407, 0.5253, 0.5102],\n",
       "         [0.3581, 0.3499, 0.2809,  ..., 0.4890, 0.5734, 0.5602],\n",
       "         [0.3659, 0.3617, 0.2940,  ..., 0.4608, 0.5509, 0.5460]],\n",
       "\n",
       "        [[0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         ...,\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(mel_specs)\n",
    "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = TensorDataset(val_spectos)\n",
    "val_loader = DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE defenition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc31 = nn.Linear(128, latent_dim)  \n",
    "        self.fc32 = nn.Linear(128, latent_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = torch.relu(self.fc1(z))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        return torch.sigmoid(self.fc3(h))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, input_dim)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='mean')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 431\n",
    "latent_dim = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "\n",
    "model = VAE(input_dim, latent_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting latent vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vectors = []\n",
    "\n",
    "model.eval()  \n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        batch = batch[0]  \n",
    "        batch = batch.view(-1, 431)  \n",
    "        mu, logvar = model.encoder(batch)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        latent_vectors.append(z)\n",
    "\n",
    "latent_vectors = torch.cat(latent_vectors, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0615,  0.7482,  0.2182,  ..., -0.3423, -1.2837, -0.5314],\n",
       "        [-0.7227, -0.7161, -0.2857,  ...,  0.9881, -1.3810, -1.3239],\n",
       "        [-0.9438, -0.7801,  0.3344,  ...,  1.4332, -1.3382, -0.8896],\n",
       "        ...,\n",
       "        [ 0.6796,  0.3389,  0.1825,  ...,  0.1115,  0.8167, -0.3806],\n",
       "        [ 0.1171, -0.1901, -0.7484,  ..., -0.3234, -0.4864,  0.0076],\n",
       "        [-0.3680, -0.1354,  0.1007,  ...,  0.1832, -0.7327,  1.7437]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vae.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under devlopment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Latent space to generate noise and decode to mel spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DDIM(nn.Module):\n",
    "    def __init__(self, latent_dim, timesteps):\n",
    "        super(DDIM, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.timesteps = timesteps\n",
    "        self.noise_schedule = self._build_noise_schedule()\n",
    "        self.noise_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "    def _build_noise_schedule(self):\n",
    "        return torch.linspace(1e-4, 0.02, self.timesteps)\n",
    "\n",
    "    def forward_diffusion(self, z):\n",
    "        noisy_z = []\n",
    "        current_z = z\n",
    "        for t in range(self.timesteps):\n",
    "            noise = torch.randn_like(current_z)\n",
    "            current_z = current_z + noise * self.noise_schedule[t]\n",
    "            noisy_z.append(current_z)\n",
    "        return noisy_z\n",
    "\n",
    "    def reverse_diffusion(self, noisy_z):\n",
    "        estimated_z = noisy_z[-1]\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            noise = torch.randn_like(estimated_z)\n",
    "            estimated_z = estimated_z - noise * self.noise_schedule[t]\n",
    "        return estimated_z\n",
    "\n",
    "    def forward(self, z):\n",
    "        noisy_z = self.forward_diffusion(z)\n",
    "        estimated_z = self.reverse_diffusion(noisy_z)\n",
    "        return estimated_z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining VAE & DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 128 * 431\n",
    "latent_dim = 64\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vae = VAE(input_dim, latent_dim)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "timesteps = 100\n",
    "ddim = DDIM(latent_dim, timesteps)\n",
    "ddim_optimizer = optim.Adam(ddim.parameters(), learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, VAE_Loss: 3.852e+06\n",
      "Epoch 0, DDIM Loss: 0.02678\n",
      "Epoch 10, VAE_Loss: 526.7\n",
      "Epoch 10, DDIM Loss: 0.02718\n",
      "Epoch 20, VAE_Loss: 1.893\n",
      "Epoch 20, DDIM Loss: 0.02688\n",
      "Epoch 30, VAE_Loss: 1.821\n",
      "Epoch 30, DDIM Loss: 0.02665\n",
      "Epoch 40, VAE_Loss: 16.87\n",
      "Epoch 40, DDIM Loss: 0.02672\n",
      "Epoch 50, VAE_Loss: 1.755\n",
      "Epoch 50, DDIM Loss: 0.0269\n",
      "Epoch 60, VAE_Loss: 1.687\n",
      "Epoch 60, DDIM Loss: 0.02737\n",
      "Epoch 70, VAE_Loss: 1.659\n",
      "Epoch 70, DDIM Loss: 0.02704\n",
      "Epoch 80, VAE_Loss: 1.606\n",
      "Epoch 80, DDIM Loss: 0.02689\n",
      "Epoch 90, VAE_Loss: 1.558\n",
      "Epoch 90, DDIM Loss: 0.02688\n",
      "Epoch 100, VAE_Loss: 1.576\n",
      "Epoch 100, DDIM Loss: 0.02681\n",
      "Epoch 110, VAE_Loss: 1.499\n",
      "Epoch 110, DDIM Loss: 0.0269\n",
      "Epoch 120, VAE_Loss: 1.453\n",
      "Epoch 120, DDIM Loss: 0.02706\n",
      "Epoch 130, VAE_Loss: 2.845\n",
      "Epoch 130, DDIM Loss: 0.0271\n",
      "Epoch 140, VAE_Loss: 1.825\n",
      "Epoch 140, DDIM Loss: 0.02675\n",
      "Epoch 150, VAE_Loss: 1.477\n",
      "Epoch 150, DDIM Loss: 0.02657\n",
      "Epoch 160, VAE_Loss: 1.274\n",
      "Epoch 160, DDIM Loss: 0.02674\n",
      "Epoch 170, VAE_Loss: 1.439\n",
      "Epoch 170, DDIM Loss: 0.02704\n",
      "Epoch 180, VAE_Loss: 1.244\n",
      "Epoch 180, DDIM Loss: 0.02701\n",
      "Epoch 190, VAE_Loss: 4.926\n",
      "Epoch 190, DDIM Loss: 0.02697\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "# VAE    \n",
    "    for batch in data_loader:\n",
    "        batch = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.view(-1, input_dim)\n",
    "        recon_batch, mu, logvar = vae(batch)\n",
    "        loss = loss_function(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, VAE_Loss: {train_loss / len(data_loader):.4}\")\n",
    "\n",
    "# DDIM\n",
    "    ddim_loss = 0\n",
    "    for batch in data_loader:\n",
    "        batch = batch[0]\n",
    "        batch = batch.view(-1, input_dim)\n",
    "        mu, logvar = vae.encoder(batch)\n",
    "        z = vae.reparameterize(mu, logvar)\n",
    "        estimated_z = ddim(z)\n",
    "        loss = F.mse_loss(estimated_z, z)\n",
    "        ddim_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        ddim_optimizer.step()\n",
    "        ddim_loss += loss.item()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, DDIM Loss: {ddim_loss / len(data_loader):.4}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), \"vae0.pt\")\n",
    "torch.save(ddim.state_dict(), \"ddim0.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DDIM(\n",
       "  (noise_predictor): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vae = VAE(input_dim, latent_dim)\n",
    "loaded_ddim = DDIM(latent_dim, timesteps)\n",
    "\n",
    "loaded_vae.load_state_dict(torch.load('vae0.pt'))\n",
    "loaded_ddim.load_state_dict(torch.load('ddim0.pt'))\n",
    "\n",
    "loaded_vae.eval()\n",
    "loaded_ddim.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reconstructing mel to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "S_dB_min = 0  \n",
    "S_dB_max = 1  \n",
    "sr = 22050  \n",
    "def denormalize_mel_spectrogram(mel_spec_tensor, S_dB_min, S_dB_max):\n",
    "    mel_spec = mel_spec_tensor.numpy()  \n",
    "    S_dB = mel_spec * (S_dB_max - S_dB_min) + S_dB_min\n",
    "    return S_dB\n",
    "\n",
    "def mel_spectrogram_to_audio(mel_spec_tensor, sr, S_dB_min, S_dB_max, n_fft=2048, hop_length=512, n_mels=128):\n",
    "    \n",
    "    S_dB = denormalize_mel_spectrogram(mel_spec_tensor, S_dB_min, S_dB_max)\n",
    "    \n",
    "    mel_basis = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)\n",
    "    inv_mel_basis = np.linalg.pinv(mel_basis)\n",
    "    \n",
    "    if S_dB.shape[0] != n_mels:\n",
    "        raise ValueError(f\"Expected mel spectrogram with {n_mels} mel bins, but got {S_dB.shape[0]}\")\n",
    "\n",
    "    linear_spec = np.dot(inv_mel_basis, librosa.db_to_power(S_dB))\n",
    "    \n",
    "    audio = librosa.griffinlim(linear_spec, hop_length=hop_length)\n",
    "    return audio\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        batch = batch[0]\n",
    "        batch = batch.view(-1, input_dim)\n",
    "        \n",
    "        mu, logvar = loaded_vae.encoder(batch)\n",
    "        z = loaded_vae.reparameterize(mu, logvar)\n",
    "        \n",
    "        estimated_z = loaded_ddim(z)\n",
    "        \n",
    "        reconstructed_batch = loaded_vae.decoder(estimated_z)\n",
    "        \n",
    "        reconstructed_audio = []\n",
    "        for i, spec in enumerate(reconstructed_batch):\n",
    "            spec = spec.view(n_mels, -1) \n",
    "            audio = mel_spectrogram_to_audio(spec, sr, S_dB_min, S_dB_max)\n",
    "            reconstructed_audio.append(audio)\n",
    "\n",
    "            sf.write(f'reconstructed_audio_{i}.wav', audio, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def extract_features(model, data_loader, device):\n",
    "    model.to(device).eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            feature = model(data)\n",
    "            features.append(feature.cpu().numpy())\n",
    "    features = np.vstack(features)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(features):\n",
    "    mean = np.mean(features, axis=0)\n",
    "    cov = np.cov(features, rowvar=False)\n",
    "    return mean, cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def frechet_distance(mu1, sigma1, mu2, sigma2):\n",
    "    diff = mu1 - mu2\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return np.dot(diff, diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your feature extraction model\n",
    "model = load_pretrained_audio_model()\n",
    "\n",
    "# Extract features\n",
    "real_features = extract_features(model, real_data_loader, device='cuda')\n",
    "generated_features = extract_features(model, generated_data_loader, device='cuda')\n",
    "\n",
    "# Compute statistics\n",
    "real_mean, real_cov = compute_statistics(real_features)\n",
    "gen_mean, gen_cov = compute_statistics(generated_features)\n",
    "\n",
    "# Compute Fréchet Distance\n",
    "fad_score = frechet_distance(real_mean, real_cov, gen_mean, gen_cov)\n",
    "print(f\"FAD score: {fad_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
