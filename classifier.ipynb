{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in /home/kuzhalogi/.local/lib/python3.10/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: torch in /home/kuzhalogi/.local/lib/python3.10/site-packages (2.3.1+cpu)\n",
      "Requirement already satisfied: torchvision in /home/kuzhalogi/.local/lib/python3.10/site-packages (0.18.1+cpu)\n",
      "Requirement already satisfied: torchaudio in /home/kuzhalogi/.local/lib/python3.10/site-packages (2.3.1+cpu)\n",
      "Requirement already satisfied: numpy in /home/kuzhalogi/.local/lib/python3.10/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/kuzhalogi/.local/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib in /home/kuzhalogi/.local/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /media/kuzhalogi/Storage/anaconda3/envs/audiogen/lib/python3.10/site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: filelock in /home/kuzhalogi/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/kuzhalogi/.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/kuzhalogi/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/kuzhalogi/.local/lib/python3.10/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in /media/kuzhalogi/Storage/anaconda3/envs/audiogen/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.3.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in /media/kuzhalogi/Storage/anaconda3/envs/audiogen/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kuzhalogi/.local/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa torch torchvision torchaudio numpy pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import torch    \n",
    "import numpy as np\n",
    "import warnings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# GTZAN Path (Update this to your dataset location)\n",
    "DATASET_PATH = \"/media/kuzhalogi/Storage/Workspace/Pakari/gtzan_dataset/Data/genres_original\"\n",
    "\n",
    "# Genre Labels\n",
    "GENRES = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to Load and Convert Audio to Mel Spectrogram\n",
    "def extract_mel_spectrogram(file_path, sr=22050, n_mels=128, hop_length=512, target_len=1293):\n",
    "    try: \n",
    "        y, _ = librosa.load(file_path, sr=sr)  # Load audio\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)  # Convert to dB scale\n",
    "\n",
    "        # Pad or truncate spectrogram to the target length\n",
    "        mel_spec_db = mel_spec_db[:, :target_len]  # Truncate\n",
    "        mel_spec_db = np.pad(mel_spec_db, ((0, 0), (0, target_len - mel_spec_db.shape[1])), mode='constant')  # Pad\n",
    "\n",
    "        return mel_spec_db  # Return as NumPy array\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Skipping file {file_path} due to error: {e}\")\n",
    "        return np.zeros((n_mels, target_len))  # Return zero spectrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom PyTorch Dataset\n",
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, dataset_path, genres, transform=None,target_len=1293):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.genres = genres\n",
    "        self.transform = transform\n",
    "        self.target_len = target_len\n",
    "        self.data = []\n",
    "\n",
    "        # Load all files\n",
    "        for genre_idx, genre in enumerate(genres):\n",
    "            genre_path = os.path.join(dataset_path, genre)\n",
    "            for file in os.listdir(genre_path):\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_path = os.path.join(genre_path, file)\n",
    "                    self.data.append((file_path, genre_idx))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.data[idx]\n",
    "        mel_spec = extract_mel_spectrogram(file_path, target_len=self.target_len)  # Convert audio to Mel Spectrogram\n",
    "\n",
    "        # Convert to Tensor\n",
    "        mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0)  # Add channel dim\n",
    "\n",
    "        return mel_spec, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = GTZANDataset(DATASET_PATH, GENRES)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Shape: torch.Size([16, 1, 128, 1293]), Label: torch.Size([16])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test the dataset\n",
    "sample_data, sample_label = next(iter(train_loader))\n",
    "print(f\"Sample Shape: {sample_data.shape}, Label: {sample_label.shape}\")\n",
    "print( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training CRNN with GTZAN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, input_channels=1, hidden_size=128, num_lstm_layers=2):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        \n",
    "        # Batch Normalization after each Conv Layer\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Maxpooling after Conv Layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=20608, hidden_size=hidden_size, num_layers=num_lstm_layers, batch_first=True)\n",
    "\n",
    "        # Fully connected layer to output classification\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through Conv Layers + MaxPool + BatchNorm\n",
    "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        print(f\"before --> {x.shape}\")\n",
    "        # [16,128,16,161]\n",
    "        # Reshape for LSTM (batch_size, seq_len, feature_size)\n",
    "        x = x.reshape(x.shape[0], x.shape[2], -1)\n",
    "        # [16, 16, 128 * 161] -> [16, 16, 20608]\n",
    "\n",
    "        # print(f\"after reshaping{x.shape}\")\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        x, (hn, cn) = self.lstm(x)\n",
    "\n",
    "        # Take the last hidden state of the LSTM for classification\n",
    "        x = hn[-1]\n",
    "\n",
    "        # Final classification\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CRNN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Loop\n",
    "def train_model(model, train_loader, epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            print(outputs.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_104382/3355933455.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, _ = librosa.load(file_path, sr=sr)  # Load audio\n",
      "/home/kuzhalogi/.local/lib/python3.10/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "/tmp/ipykernel_104382/3355933455.py:15: UserWarning: Skipping file /media/kuzhalogi/Storage/Workspace/Pakari/gtzan_dataset/Data/genres_original/jazz/jazz.00054.wav due to error: \n",
      "  warnings.warn(f\"Skipping file {file_path} due to error: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([8, 128, 16, 161])\n",
      "torch.Size([8, 10])\n",
      "Epoch 1/10, Loss: 2.2395\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n",
      "before --> torch.Size([16, 128, 16, 161])\n",
      "torch.Size([16, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      6\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      8\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      9\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36mGTZANDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     22\u001b[0m     file_path, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m---> 23\u001b[0m     mel_spec \u001b[38;5;241m=\u001b[39m \u001b[43mextract_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_len\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert audio to Mel Spectrogram\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Convert to Tensor\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     mel_spec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mel_spec, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add channel dim\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mextract_mel_spectrogram\u001b[0;34m(file_path, sr, n_mels, hop_length, target_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_mel_spectrogram\u001b[39m(file_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22050\u001b[39m, n_mels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, hop_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, target_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1293\u001b[39m):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m----> 4\u001b[0m         y, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m(file_path, sr\u001b[38;5;241m=\u001b[39msr)  \u001b[38;5;66;03m# Load audio\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         mel_spec \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmelspectrogram(y\u001b[38;5;241m=\u001b[39my, sr\u001b[38;5;241m=\u001b[39msr, n_mels\u001b[38;5;241m=\u001b[39mn_mels, hop_length\u001b[38;5;241m=\u001b[39mhop_length)\n\u001b[1;32m      6\u001b[0m         mel_spec_db \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpower_to_db(mel_spec, ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax)  \u001b[38;5;66;03m# Convert to dB scale\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/lazy_loader/__init__.py:77\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     71\u001b[0m attr_to_modules \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     72\u001b[0m     attr: mod \u001b[38;5;28;01mfor\u001b[39;00m mod, attrs \u001b[38;5;129;01min\u001b[39;00m submod_attrs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attrs\n\u001b[1;32m     73\u001b[0m }\n\u001b[1;32m     75\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(submodules \u001b[38;5;241m|\u001b[39m attr_to_modules\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m submodules:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the Model\n",
    "train_model(model, train_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# def evaluate_model(model, test_loader):\n",
    "#     model.eval()\n",
    "#     all_preds, all_labels = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             inputs, labels = batch\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "\n",
    "#             all_preds.extend(preds.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#     accuracy = accuracy_score(all_labels, all_preds)\n",
    "#     print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Call Evaluation Function\n",
    "# evaluate_model(model, train_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
